{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Word Vectors?\n",
    "\n",
    "* Many Machine Learning algorithms and almost all Deep Learning Architectures are incapable of processing strings or plain text in their raw form. They require numbers as inputs to perform any sort of job, be it classification, regression etc. in broad terms. \n",
    "\n",
    "* Word Vectors are the texts converted into numbers and there may be different numerical representations of the same text.\n",
    "\n",
    "* Example:\n",
    "\n",
    "\n",
    "     [‘Word’,’Embeddings’,’are’,’Converted’,’into’,’numbers’] --> [0,0,0,1,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of text documents\n",
    "corpus = [\n",
    "     'This is the first document.',\n",
    "     'This document is the second document.',\n",
    "     'And this is the third one.',\n",
    "     'Is this the first document?',\n",
    " ]\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer(analyzer='word')\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(corpus)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "# encode document\n",
    "vector = vectorizer.transform(corpus)\n",
    "# summarize encoded vector\n",
    "#print(vector.shape)\n",
    "#print(type(vector))\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\",\n",
    "        \"The dog.\",\n",
    "        \"The fox\"]\n",
    "\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.idf_)\n",
    "# encode document\n",
    "\n",
    "vector = vectorizer.transform([text[0]])\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())\n",
    "vector1 = vectorizer.transform([text[1]])\n",
    "print(vector1.shape)\n",
    "print(vector1.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code-Mixed Language Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Read Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_No</th>\n",
       "      <th>Words</th>\n",
       "      <th>POS_Tags</th>\n",
       "      <th>LI_Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>T'wood</td>\n",
       "      <td>N_NN</td>\n",
       "      <td>univ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>RD_PUNC</td>\n",
       "      <td>univ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>We</td>\n",
       "      <td>PR_PRP</td>\n",
       "      <td>univ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>have</td>\n",
       "      <td>V_VM</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>craze</td>\n",
       "      <td>N_NN</td>\n",
       "      <td>univ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence_No   Words POS_Tags LI_Tags\n",
       "0            1  T'wood     N_NN    univ\n",
       "1            1       -  RD_PUNC    univ\n",
       "2            1      We   PR_PRP    univ\n",
       "3            1    have     V_VM      en\n",
       "4            1   craze     N_NN    univ"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Words = []\n",
    "LI_Tags = []\n",
    "POS_Tags = []\n",
    "Sent_ID = []\n",
    "\n",
    "count=1\n",
    "\n",
    "#Reading data file\n",
    "\n",
    "dataFile = open(\"LIDataset/CodemixedShuffle.txt\",\"r\")\n",
    "fileData = dataFile.readlines()\n",
    "for line in fileData:\n",
    "    a = line.split(\"\\t\")\n",
    "    if(len(a)==3):\n",
    "        Words.append(a[0].strip())\n",
    "        POS_Tags.append(a[2].strip()) \n",
    "        LI_Tags.append(a[1].strip())\n",
    "        Sent_ID.append(count)\n",
    "    else:\n",
    "        count+=1\n",
    "        \n",
    "# Converting data file to data frame\n",
    "\n",
    "codeMixedData = pd.DataFrame(list(zip(Sent_ID, Words, POS_Tags, LI_Tags)),\n",
    "               columns =['Sentence_No', 'Words', 'POS_Tags', 'LI_Tags'])\n",
    "        \n",
    "codeMixedData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['univ', 'en', 'te', 'ne', 'DM_DMD', 'N_NN', 'RB_AMN'], dtype=object)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"LIDataset/CodemixedShuffle.txt\", header=None, sep=\"\\\\t\", names=['word', 'language', 'pos'], engine='python')\n",
    "df['language'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Splitting and Label Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15412    univ\n",
      "14857      te\n",
      "23349      en\n",
      "18819      te\n",
      "18932    univ\n",
      "         ... \n",
      "11782      en\n",
      "5676       en\n",
      "10950      en\n",
      "15436    univ\n",
      "16511      en\n",
      "Name: LI_Tags, Length: 5900, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(codeMixedData['Words'], codeMixedData['LI_Tags'], test_size=0.2, random_state=42)\n",
    "print(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  4.1 Feature Engineering:  Count Vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5900, 6440), (23596, 6440))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer(min_df=1)\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.fit_transform(train_x).toarray()\n",
    "xvalid_count =  count_vect.transform(test_x).toarray()\n",
    "\n",
    "xvalid_count.shape, xtrain_count.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  4.2 Feature Engineering:  TF-IDF Vectorizer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(min_df=1)\n",
    "tfidf_vect.fit(text)\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(test_x)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(min_df=1)\n",
    "tfidf_vect_ngram.fit(text)\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(test_x)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(min_df=1)\n",
    "tfidf_vect_ngram_chars.fit(text)\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(test_x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    acc = metrics.accuracy_score(predictions, test_y)\n",
    "    f1 = metrics.f1_score(predictions, test_y, average='weighted')\n",
    "    print(classification_report(predictions, test_y))\n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          en       0.75      0.83      0.79      1552\n",
      "          ne       0.13      0.67      0.22        30\n",
      "          te       0.67      0.84      0.75      1440\n",
      "        univ       0.81      0.62      0.70      2878\n",
      "\n",
      "    accuracy                           0.73      5900\n",
      "   macro avg       0.59      0.74      0.61      5900\n",
      "weighted avg       0.76      0.73      0.73      5900\n",
      "\n",
      "LR, WordLevel TF-IDF:  0.7316949152542372 0.7341706331946041\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          en       0.75      0.83      0.79      1552\n",
      "          ne       0.13      0.67      0.22        30\n",
      "          te       0.67      0.84      0.75      1440\n",
      "        univ       0.81      0.62      0.70      2878\n",
      "\n",
      "    accuracy                           0.73      5900\n",
      "   macro avg       0.59      0.74      0.61      5900\n",
      "weighted avg       0.76      0.73      0.73      5900\n",
      "\n",
      "LR, N-Gram Vectors:  0.7316949152542372 0.7341706331946041\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          en       0.75      0.83      0.79      1552\n",
      "          ne       0.13      0.67      0.22        30\n",
      "          te       0.67      0.84      0.75      1440\n",
      "        univ       0.81      0.62      0.70      2878\n",
      "\n",
      "    accuracy                           0.73      5900\n",
      "   macro avg       0.59      0.74      0.61      5900\n",
      "weighted avg       0.76      0.73      0.73      5900\n",
      "\n",
      "LR, CharLevel Vectors:  0.7316949152542372 0.7341706331946041\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Count Vectors\n",
    "#accuracy, f1_score = train_model(linear_model.LogisticRegression(solver='lbfgs', max_iter=1000), xtrain_count, train_y, xvalid_count)\n",
    "#print(\"LR, Count Vectors: \", accuracy, f1_score)\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(linear_model.LogisticRegression(solver='lbfgs', max_iter=1000), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"LR, WordLevel TF-IDF: \", accuracy, f1_score)\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(linear_model.LogisticRegression(solver='lbfgs', max_iter=1000), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print(\"LR, N-Gram Vectors: \", accuracy, f1_score)\n",
    "\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(linear_model.LogisticRegression(solver='lbfgs', max_iter=1000), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print(\"LR, CharLevel Vectors: \", accuracy, f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "labels = []\n",
    "\n",
    "dataFile = open(\"LIDataset/shuffle_data.txt\",\"r\")\n",
    "fileData = dataFile.readlines()\n",
    "for line in fileData:\n",
    "    a = line.strip().split(\"\\t\")\n",
    "    if(len(a)==14):\n",
    "        features = a[:-1]\n",
    "        features.append(str(len(a[0])))\n",
    "        text.append(features)\n",
    "        labels.append(a[-1])\n",
    "        \n",
    "\n",
    "text = [' '.join(i) for i in text]  \n",
    "\n",
    "text = np.asarray(text)\n",
    "labels = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['thaman 1 0 0 0 0 G_N t th tha n an man 6',\n",
       "       'skn 1 0 0 0 0 G_N s sk skn n kn skn 3',\n",
       "       'ni 1 0 0 0 0 G_N n ni null i ni null 2',\n",
       "       'thosesthe 1 0 0 0 0 G_X t th tho e he the 9',\n",
       "       'asalu 1 0 0 0 0 G_X a as asa u lu alu 5',\n",
       "       'stage 1 0 0 0 0 G_N s st sta e ge age 5',\n",
       "       'ninchi 1 0 0 0 0 G_X n ni nin i hi chi 6',\n",
       "       'audience 1 0 0 0 0 G_N a au aud e ce nce 8',\n",
       "       'seat 1 0 0 0 0 G_N s se sea t at eat 4',\n",
       "       'deggara 1 0 0 0 0 G_X d de deg a ra ara 7'], dtype='<U160')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(text, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(min_df=1, analyzer='word', max_features=25000)\n",
    "tfidf_vect.fit(text)\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(test_x)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', ngram_range=(2,3), max_features=25000)\n",
    "tfidf_vect_ngram.fit(text)\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(test_x)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', ngram_range=(2,3), max_features=25000)\n",
    "tfidf_vect_ngram_chars.fit(text)\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(test_x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          en       0.90      0.89      0.90      2887\n",
      "          ne       0.28      0.66      0.39        44\n",
      "          te       0.89      0.85      0.87      1666\n",
      "        univ       0.69      0.70      0.70      1289\n",
      "\n",
      "    accuracy                           0.84      5886\n",
      "   macro avg       0.69      0.78      0.71      5886\n",
      "weighted avg       0.85      0.84      0.84      5886\n",
      "\n",
      "LR, WordLevel TF-IDF:  0.8386000679578661 0.841574259541367\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          en       0.91      0.88      0.90      2978\n",
      "          ne       0.28      0.63      0.38        46\n",
      "          te       0.90      0.88      0.89      1635\n",
      "        univ       0.67      0.72      0.69      1227\n",
      "\n",
      "    accuracy                           0.84      5886\n",
      "   macro avg       0.69      0.78      0.72      5886\n",
      "weighted avg       0.85      0.84      0.85      5886\n",
      "\n",
      "LR, N-Gram Vectors:  0.8433571185864764 0.8474631517301036\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          en       0.96      0.93      0.94      2969\n",
      "          ne       0.30      0.64      0.41        50\n",
      "          te       0.92      0.86      0.89      1729\n",
      "        univ       0.75      0.87      0.81      1138\n",
      "\n",
      "    accuracy                           0.89      5886\n",
      "   macro avg       0.73      0.82      0.76      5886\n",
      "weighted avg       0.90      0.89      0.90      5886\n",
      "\n",
      "LR, CharLevel Vectors:  0.8917770981991165 0.8957562452732752\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(linear_model.LogisticRegression(solver='lbfgs', max_iter=1000), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"LR, WordLevel TF-IDF: \", accuracy, f1_score)\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(linear_model.LogisticRegression(solver='lbfgs', max_iter=1000), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print(\"LR, N-Gram Vectors: \", accuracy, f1_score)\n",
    "\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(linear_model.LogisticRegression(solver='lbfgs', max_iter=1000), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print(\"LR, CharLevel Vectors: \", accuracy, f1_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
